from skimage.color import rgb2gray
from skimage.transform import resize
import tensorflow as tf
import numpy as np
import threading
import random
import scipy.signal
import time
from time import sleep
import tensorflow.contrib.slim as slim
import gym
import logging
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
global episode
global avg_pmaxs
episode=0
EPISODES=10000
env_name="BreakoutDeterministic-v4" #4개중의 3개 skiipped frame 제공
scores=[]
avg_pmaxs=[]
# color images -> bw images value 0~255
def pre_processing(next_observe, observe):
    processed_observe=np.maximum(next_observe,observe) 
    processed_observe= resize(rgb2gray(processed_observe),(84,84),mode='constant')
    return processed_observe

def update_target_graph(from_scope,to_scope):
    from_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, from_scope)
    to_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, to_scope)

    op_holder = []
    for from_var,to_var in zip(from_vars,to_vars):
        op_holder.append(to_var.assign(from_var))
    return op_holder

# Discounting function used to calculate discounted returns.
def discount(x, gamma):
    return scipy.signal.lfilter([1], [1, -gamma], x[::-1], axis=0)[::-1]

#Used to initialize weights for policy and value output layers
def normalized_columns_initializer(std=1.0):
    def _initializer(shape, dtype=None, partition_info=None):
        out = np.random.randn(*shape).astype(np.float32)
        out *= std / np.sqrt(np.square(out).sum(axis=0, keepdims=True))
        return tf.constant(out)
    return _initializer

class AC_Network():
    def __init__(self,state_size,action_size,scope,trainer):
        with tf.variable_scope(scope):
            self.inputs=tf.placeholder(shape=state_size,dtype=tf.float32,name='state_input')
            
            #image process
            #self.imageIn = tf.reshape(self.inputs,shape=[-1,84,84,1])
            with slim.arg_scope([slim.conv2d, slim.fully_connected], 
                                weights_initializer=slim.xavier_initializer(),
                                activation_fn=tf.nn.relu):
                self.conv1 = slim.conv2d(inputs=self.inputs,num_outputs=16,
                    kernel_size=[8,8],stride=[4,4],padding='VALID')
                self.conv2 = slim.conv2d(inputs=self.conv1,num_outputs=32,
                    kernel_size=[4,4],stride=[2,2],padding='VALID')
                hidden = slim.fully_connected(slim.flatten(self.conv2),256)

                 #Output layers for policy and value estimations
                self.logits = slim.fully_connected(hidden,action_size,
                    activation_fn=None)
                self.policy=tf.nn.softmax(self.logits,name='policy')
                self.value = slim.fully_connected(hidden,1,
                    activation_fn=None,scope='value')
            
            #Only the worker network need ops for loss functions and gradient updating.
            if scope != 'global':
                self.actions = tf.placeholder(shape=[None],dtype=tf.int32)
                self.actions_onehot = tf.one_hot(self.actions,action_size,dtype=tf.float32,name='action_onehot')
                self.target_v = tf.placeholder(shape=[None],dtype=tf.float32,name='target_v')
                self.advantages = tf.placeholder(shape=[None],dtype=tf.float32,name='advantages')

                action_prob = tf.reduce_sum(self.policy * self.actions_onehot, [1])

                #Loss functions
                self.value_loss = 0.5 * tf.reduce_sum(tf.square(self.target_v - tf.reshape(self.value,[-1])),name='value_loss')
                
                self.entropy =  tf.reduce_sum(self.policy * tf.log(self.policy+1.e-10),name='entropy')
                self.policy_loss = -tf.reduce_sum(tf.log(action_prob+1.e-10)*self.advantages,name='policy_loss') #cross_entropy
                self.loss = 0.5 * self.value_loss + self.policy_loss +self.entropy * 0.01
                #self.loss=tf.add_n([0.5*self.value_loss,self.policy_loss,+self.entropy*0.01],name='loss')

                #Get gradients from local network using local losses , policy, value network를 따로따로 update를 안하네??
                local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)
                self.gradients = tf.gradients(self.loss,local_vars)
                self.var_norms = tf.global_norm(local_vars,name='var_norms')
                grads,self.grad_norms = tf.clip_by_global_norm(self.gradients,40.0)
                grads=self.gradients
                #Apply local gradients to global network
                global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, 'global')
                self.apply_grads = trainer.apply_gradients(zip(grads,global_vars))            
class Worker():
    def __init__(self,name,state_size,action_size,trainer,model_path):
        self.name = "worker_" + str(name)
        self.number = name        
        self.model_path = model_path
        self.trainer = trainer
        self.state_size=state_size
 #       self.global_episodes = global_episodes
 #       self.increment = self.global_episodes.assign_add(1)
        self.episode_rewards = []
        self.episode_lengths = []
        self.episode_mean_values = []
        self.summary_writer = tf.summary.FileWriter("train_"+str(self.number))

        #Create the local copy of the network and the tensorflow op to copy global paramters to local network
        self.local_AC = AC_Network(state_size,action_size,self.name,trainer)
        
        self.avg_p_max=0
        self.avg_loss=0
        self.t_max=30
        self.t=0
        
        self.env_name=env_name
        
    def get_action(self,history,sess):
        feed_dict={self.local_AC.inputs:history}
        policy,value=sess.run([self.local_AC.policy,self.local_AC.value],feed_dict=feed_dict) 
        action_index=np.random.choice(action_size,1,p=policy[0])[0] #action 종류별로 확률 p를 주고 1개의 action 선택
        return action_index,policy[0],value[0][0]
        
    def discounted_prediction(self, rewards, gamma,value,done):
        discounted_prediction = np.zeros_like(rewards)
        running_add = 0

        if not done:
            running_add = value

        for t in reversed(range(0, len(rewards))):
            running_add = running_add * gamma+ rewards[t]
            discounted_prediction[t] = running_add
        return discounted_prediction        
        
    def work(self,gamma,sess,coord,saver): #개별 agent 환경과 reaction
        global episode
        global avg_pmaxs
        env=gym.make(self.env_name)
        step = 0
        print ("Starting worker " + str(self.number))  
        update_local_net=update_target_graph('global',self.name)  

        with sess.as_default(), sess.graph.as_default():                 
            while not (coord.should_stop() or episode >= EPISODES):
                sess.run([update_local_net])
                episode_buffer = []
                episode_values = []
                episode_frames = []
                episode_reward = 0
                episode_step_count = 0
                done = False
                dead=False
                score, start_life=0,5
                observe=env.reset()
                next_observe=observe
                
                # pending for counting random number of 0~30 
                for _ in range(random.randint(1,30)):
                    observe=next_observe
                    next_observe,_,_,_=env.step(1)
                    
                state=pre_processing(next_observe,observe)
                history=np.stack((state,state,state,state),axis=2)    
                history=np.reshape(history,(1,84,84,4))
                
                
                while not done:
                    step+=1
                    self.t+=1 
                    observe=next_observe
                    action,policy,value= self.get_action(history,sess)

                    if dead:
                        action=0
                        dead=False
                    real_action=action+1
                    
                    next_observe,reward,done,info=env.step(real_action) # 새로운 step 진행
                    
                    next_state=pre_processing(next_observe,observe) #color image -> bw image
                    next_state=np.reshape([next_state],(1,84,84,1))
                    next_history=np.append(next_state,history[:,:,:,:3],axis=3) #현재 장면에 과거 3장면 붙이기

                    self.avg_p_max+=np.amax(policy)
                    
                    if start_life>info['ale.lives']:
                        dead=True
                        start_life=info['ale.lives']
                        
                    score+=reward
                    reward=np.clip(reward,-1,1)
                    history2=np.reshape(history,(84,84,4))
                    next_history2=np.reshape(next_history,(84,84,4))
                    episode_buffer.append([history2,action,reward,next_history2,done,value])
                    
                    
                    if dead: #죽으면 그 상태에서 다시 초기화
                        history=np.stack((next_state,next_state,next_state,next_state),axis=2)  
                        history=np.reshape([history],(1,84,84,4))
                    else:
                        history=next_history
                        
                    if self.t>=self.t_max or done: #episode 끝나거나 목표 frame 수까지 진행되었으면 net update
                        feed_dict={self.local_AC.inputs:history}
                        value_p1=sess.run(self.local_AC.value,feed_dict=feed_dict)[0][0]       
                        v_l,p_l,e_l,g_n,v_n=self.train(episode_buffer,sess,gamma,value_p1)
                        sess.run([update_local_net])
                        episode_buffer=[]
                        self.t=0
                        
                    if done:
                        episode +=1
                        print("episode:",episode, " score:",score, " step:", step)

                        scores.append(score)
                        avg_pmaxs.append(self.avg_p_max/float(step))
                        self.avg_p_max=0
                        stats=[episode, score, self.avg_p_max/float(step),step] 
                        step=0
                        logger.info('episode: %5d, score: %7.4f, avg_p_max: %7.4f'%(episode,score, avg_pmaxs[-1]))
                               
    def train(self,rollout,sess,gamma,bootstrap_value):
        rollout = np.array(rollout)
        states = rollout[:,0]
        actions = rollout[:,1] #k,
        rewards = rollout[:,2] #k,
        next_history = rollout[:,3]
        dones=rollout[:,4]
        values = rollout[:,5]        
        
        states=np.stack(states,axis=0)

       
        self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value]) 

        discounted_rewards=self.discounted_prediction(rewards,gamma,bootstrap_value,dones[-1])
        self.value_plus = np.asarray(values.tolist() + [bootstrap_value]) 

        advantages = discounted_rewards - values
        testa=self.value_plus[1:]

        feed_dict = {self.local_AC.target_v:discounted_rewards,
            self.local_AC.inputs:states,
            self.local_AC.actions:actions,
            self.local_AC.advantages:advantages}
        val_loss,policy_loss,entropy_loss,grad_norm,var_norm,_ = sess.run([self.local_AC.value_loss,
            self.local_AC.policy_loss, self.local_AC.entropy, self.local_AC.grad_norms,
            self.local_AC.var_norms, self.local_AC.apply_grads], feed_dict=feed_dict)
        samp_size=len(rollout)
        return val_loss/samp_size,policy_loss /samp_size,entropy_loss /samp_size, grad_norm,var_norm  
 
gamma = .99 # discount rate for advantage estimation and reward discounting
#state_size = 84*84 # Observations are greyscale frames of 84 * 84 * 1
state_size=(None,84,84,4)
action_size = 3 # Agent can move Left, Right, or Fire
load_model = False
model_path = './model'

if not os.path.exists(model_path):
    os.makedirs(model_path)
    
#Create a directory to save episode playback gifs to
if not os.path.exists('./frames'):
    os.makedirs('./frames')

# create a logger 
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
# create file handler which logs even debug messages
fh = logging.FileHandler(model_path+'A3C_BreakOut'+'.log')
fh.setLevel(logging.DEBUG)
# create console handler with a higher log level

formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
fh.setFormatter(formatter)

# add the handlers to the logger
logger.addHandler(fh)

tf.reset_default_graph()

with tf.device("/cpu:0"): 
    #trainer = tf.train.AdamOptimizer(learning_rate=1e-4)
    trainer=tf.train.RMSPropOptimizer(learning_rate=2.5e-4)
    master_network = AC_Network(state_size,action_size,'global',None) # Generate global network
    num_workers = 8
    workers = []
    # Create worker classes
    for i in range(num_workers):
        workers.append(Worker(i,state_size,action_size,trainer,model_path))
    saver = tf.train.Saver(max_to_keep=5)

with tf.Session() as sess:
    coord = tf.train.Coordinator()
    if load_model == True:
        print ('Loading Model...')
        ckpt = tf.train.get_checkpoint_state(model_path)
        saver.restore(sess,ckpt.model_checkpoint_path)
    else:
        sess.run(tf.global_variables_initializer())
        
    # This is where the asynchronous magic happens.
    # Start the "work" process for each worker in a separate threat.
    worker_threads = []
    for worker in workers:
        #worker_work = lambda: worker.work(max_episode_length,gamma,sess,coord,saver)
        t = threading.Thread(target=(worker.work),args=(gamma,sess,coord,saver))
        t.start()
        sleep(0.5)
        worker_threads.append(t)
    coord.join(worker_threads)
